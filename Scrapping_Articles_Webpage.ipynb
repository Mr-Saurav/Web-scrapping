{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b4e05f-26bf-4b79-b7bd-4905c6254a0a",
   "metadata": {},
   "source": [
    "# Article Webpages Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6c4846-b837-45d3-a027-03d617c6a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086024c7-9b4f-41b7-bc75-8cf0a0331429",
   "metadata": {},
   "source": [
    "##### REQUIREMENTS: \n",
    "1. All Webpages must be article/blog webpage. And all should be the same company webpages.\n",
    "2. Need to upload the \"Input_file\"(excel file which contain the links of webpages of particular company) inside environment. \n",
    "3. Provide the number of wepages link that excel file contain in \"num_of_files\".\n",
    "4. Provide \"folder_name\" (make sure folder is create first at environment with same name). Here, we give \"extracted_articles\" as folder name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e723af37-751a-4af5-94cc-1dbdb944cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load the input.xlsx file inside python environment\n",
    "\n",
    "def store_articles(Input_file, num_of_files, folder_name):\n",
    "    \n",
    "    # make a dataframe where we load data present in input file\n",
    "    input_df = pd.read_excel(Input_file)\n",
    "\n",
    "    # through iteration we get url from dataframe we made\n",
    "    for i in range(num_of_files):\n",
    "        url = input_df.URL[i]\n",
    "        page = requests.get(url)\n",
    "\n",
    "        # using BeautifulSoup for scraping the data\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        #change the class according to the webpage\n",
    "        title = soup.find('h1', attrs={'class': 'entry-title'})\n",
    "        if title is None:\n",
    "            title = soup.find('h1', attrs={'class':'tdb-title-text'})\n",
    "\n",
    "        content = soup.find('div', attrs={'class': 'td-post-content tagdiv-type'})\n",
    "        if content is None:\n",
    "            content = soup.find('div', attrs={'class': 'tdb-block-inner td-fix-index'})\n",
    "\n",
    "        # check if title and content are found\n",
    "        if title is not None and content is not None:\n",
    "            f_title = title.text\n",
    "            f_content = content.text.replace('\\n', '')\n",
    "\n",
    "            # storing each Article in folder \"extracted_articles\" with name \"webpage0001, webpage0002,....,etc\"\n",
    "            name = folder_name + '/webpage' + str(\"{:04d}\".format(i+1)) + '.txt'\n",
    "            with open(name, \"w\") as file:\n",
    "                file.write(f_title + \"\\n\\n\" + f_content)\n",
    "                file.flush()\n",
    "        else:\n",
    "            print(f'Webpage {str(\"{:04d}\".format(i+1))} have 404 Error')\n",
    "            \n",
    "    print(f\"\\n\\n All {num_of_files} webpages loaded as txt in {folder_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a2cf6b-3bbf-481b-ba1d-68601cb350b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage 0036 have 404 Error\n",
      "Webpage 0049 have 404 Error\n",
      "\n",
      "\n",
      " All 100 webpages loaded as txt in extracted_articles\n"
     ]
    }
   ],
   "source": [
    "#main function\n",
    "if __name__ == \"__main__\" :\n",
    "    #load the input.xlsx file inside python environment            \n",
    "    #[Note: Each articles store inside \"extracted_articles\" folder]\n",
    "    Input_file = 'Input.xlsx'\n",
    "    num_of_files = 100\n",
    "    folder_name = 'extracted_articles'\n",
    "    store_articles(Input_file, num_of_files, folder_name) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
